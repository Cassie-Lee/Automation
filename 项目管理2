项目10：清理文件
1、选择性拷贝
编写一个程序，遍历一个目录树，查找特定扩展名的文件（诸如.pdf 或.jpg）。不论这些文件的位置在哪里，将它们拷贝到一个新的文件夹中。
2、删除不需要的文件
一些不需要的、巨大的文件或文件夹占据了硬盘的空间，这并不少见。如果你试图释放计算机上的空间，那么删除不想要的巨大文件效果最好。但首先你必须找
到它们。编写一个程序，遍历一个目录树，查找特别大的文件或文件夹，比方说，超过100MB 的文件（回忆一下，要获得文件的大小，可以使用 os 模块的
os.path.getsize()）。将这些文件的绝对路径打印到屏幕上。
3、消除缺失的编号
编写一个程序，在一个文件夹中，找到所有带指定前缀的文件，诸如spam001.txt,spam002.txt 等，并定位缺失的编号（例如存在spam001.txt 和spam003.txt，
但不存在spam002.txt）。让该程序对所有后面的文件改名，消除缺失的编号。作为附加的挑战，编写另一个程序，在一些连续编号的文件中，空出一些编号，
以便加入新的文件。

从Web 抓取信息：
少数可怕的时候，我没有 Wi-Fi。这时才意识到，我在计算机上所做的事，有多少实际上是在因特网上做的事。完全出于习惯，我会发现自己尝试收邮件、阅读朋友的
推特，或回答问题：“在Kurtwood Smith演出1987 年的机械战警之前，曾经演过主角吗？”因为计算机上如此多的工作都与因特网有关，所以如果程序能上网就太好了。
“Web 抓取”是一个术语，即利用程序下载并处理来自Web 的内容。例如，Google 运行了许多web 抓取程序，对网页进行索引，实现它的搜索引擎。
webbrowser：是Python 自带的，打开浏览器获取指定页面。
requests：从因特网上下载文件和网页。
Beautiful Soup：解析HTML，即网页编写的格式。
selenium：启动并控制一个Web 浏览器。selenium 能够填写表单，并模拟鼠标在这个浏览器中点击。

项目11：利用webbrowser 模块的mapIt.py
webbrowser 模块的open()函数可以启动一个新浏览器，打开指定的URL。在交互式环境中输入以下代码：
>>> import webbrowser
>>> webbrowser.open('http://inventwithpython.com/')
Web 浏览器的选项卡将打开URL http://inventwithpython.com/。这大概就是webbrowser 模块能做的唯一的事情。既使如此，open()函数确实让一些有趣的事情
成为可能。例如，将一条街道的地址拷贝到剪贴板，并在Google 地图上打开它的地图，这是很繁琐的事。你可以让这个任务减少几步，写一个简单的脚本，利用剪贴
板中的内容在浏览器中自动加载地图。这样，你只要将地址拷贝到剪贴板，运行该脚本，地图就会加载。
你的程序需要做到：
• 从命令行参数或剪贴板中取得街道地址。
• 打开Web 浏览器，指向该地址的Google 地图页面。
这意味着代码需要做下列事情：
• 从sys.argv 读取命令行参数。
• 读取剪贴板内容。
• 调用webbrowser.open()函数打开外部浏览器。
打开一个新的文件编辑器窗口，将它保存为mapIt.py。
第1 步：弄清楚URL
根据附录B 中的指导，建立mapIt.py，这样当你从命令行运行它时，例如
C:\> mapit 870 Valencia St, San Francisco, CA 94110
该脚本将使用命令行参数，而不是剪贴板。如果没有命令行参数，程序就知道
要使用剪贴板的内容。
首先你需要弄清楚，对于指定的街道地址，要使用怎样的URL。你在浏览器中打开http://maps.google.com/并查找一个地址时，地址栏中的URL 看起来就像这样：
https://www.google.com/maps/place/870+Valencia+St/@37.7590311,-122.4215096, 17z/data=!3m1!4b1!4m2!3m1!1s0x808f7e3dadc07a37:0xc86b0b
2bb93b73d8.地址就在URL 中，但其中还有许多附加的文本。网站常常在URL 中添加额外的数据，帮助追踪访问者或定制网站。但如果你尝试使用https://www.goo
gle.com/maps/place/870+Valencia+St+San+Francisco+CA/，会发现仍然可以到达正确的页面。所以你的程序可以设置为打开一个浏览器，访问'https://www.
google.com/maps/place/your_address_string'（其中your_address_string 是想查看地图的地址）。
第2 步：处理命令行参数
让你的代码看起来像这样：
#! python3
# mapIt.py - Launches a map in the browser using an address from the
# command line or clipboard.
import webbrowser, sys
if len(sys.argv) > 1:
# Get address from command line.
address = ' '.join(sys.argv[1:])
# TODO: Get address from clipboard.
在程序的#!行之后，需要导入webbrowser 模块，用于加载浏览器；导入sys 模块，用于读入可能的命令行参数。sys.argv 变量保存了程序的文件名和命令行参数
的列表。如果这个列表中不只有文件名，那么len(sys.argv)的返回值就会大于1，这意味着确实提供了命令行参数。命令行参数通常用空格分隔，但在这个例子中，
你希望将所有参数解释为一个字符串。因为sys.argv 是字符串的列表，所以你可以将它传递给join()方法，这将返回一个字符串。你不希望程序的名称出现在这个
字符串中，所以不是使用sys.argv，而是使用sys.argv[1:]，砍掉这个数组的第一个元素。这个表达式求值得到的字符串，保存在address 变量中。
如果运行程序时在命令行中输入以下内容：
mapit 870 Valencia St, San Francisco, CA 94110
…sys.argv 变量将包含这样的列表值：
['mapIt.py', '870', 'Valencia', 'St, ', 'San', 'Francisco, ', 'CA', '94110']
address 变量将包含字符串'870 Valencia St, San Francisco, CA 94110'。
第3 步：处理剪贴板内容，加载浏览器
让你的代码看起来像这样：
#! python3
# mapIt.py - Launches a map in the browser using an address from the
# command line or clipboard.
import webbrowser, sys, pyperclip
if len(sys.argv) > 1:
# Get address from command line.
address = ' '.join(sys.argv[1:])
else:
# Get address from clipboard.
address = pyperclip.paste()
webbrowser.open('https://www.google.com/maps/place/' + address)
如果没有命令行参数，程序将假定地址保存在剪贴板中。可以用pyperclip.paste()取得剪贴板的内容，并将它保存在名为address 的变量中。最后，启动外部
浏览器访问Google 地图的URL，调用webbrowser.open()。虽然你写的某些程序将完成大型任务，为你节省数小时的时间，但使用一个程序，在每次执行一个常
用任务时节省几秒钟时间，比如取得一个地址的地图，这同样令人满意。表11-1 比较了有mapIt.py 和没有它时，显示地图所需的步骤。
手工取得地图 利用mapIt.py
高亮标记地址 高亮标记地址
拷贝地址 拷贝地址
打开Web 浏览器 运行mapIt.py
打开http://maps.google.com/
点击地址文本字段
拷贝地址
按回车
看到程序让这个任务变得不那么繁琐了吗？
第4 步：类似程序的想法
只要你有一个URL，webbrowser 模块就让用户不必打开浏览器，而直接加载一个网站。其他程序可以利用这项功能完成以下任务：
• 在独立的浏览器标签中，打开一个页面中的所有链接。
• 用浏览器打开本地天气的URL。
• 打开你经常查看的几个社交网站。

项目12：用requests 模块从Web 下载文件
requests 模块让你很容易从Web 下载文件，不必担心一些复杂的问题，诸如网络错误、连接问题和数据压缩。requests 模块不是Python 自带的，所以必须先安装。
通过命令行，运行pip install requests（附录A 详细介绍了如何安装第三方模块）。编写requests 模块是因为Python 的urllib2 模块用起来太复杂。实际上，
请拿一支记号笔涂黑这一段。忘记我曾提到urllib2。如果你需要从Web 下载东西，使用requests 模块就好了。
接下来，做一个简单的测试，确保requests 模块已经正确安装。在交互式环境中输入以下代码：
>>> import requests
如果没有错误信息显示，requests 模块就已经安装成功了。
用requests.get()函数下载一个网页
requests.get()函数接受一个要下载的URL 字符串。通过在requests.get()的返回值上调用type()，你可以看到它返回一个Response 对象，其中包含了Web 
服务器对你的请求做出的响应。稍后我将更详细地解释Response 对象，但现在请在交互式环境中输入以下代码，并保持计算机与因特网的连接：
>>> import requests
 >>> res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt')
>>> type(res)
<class 'requests.models.Response'>
 >>> res.status_code == requests.codes.ok
True
>>> len(res.text)
178981
>>> print(res.text[:250])
The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare
This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever. You may copy it, give it away or
re-use it under the terms of the Proje
该URL 指向一个文本页面，其中包含整部罗密欧与朱丽叶，它是由古登堡计划提供的。通过检查Response 对象的status_code 属性，你可以了解对这个网页的
请求是否成功。如果该值等于requests.codes.ok，那么一切都好（顺便说一下，HTTP协议中“OK”的状态码是200。你可能已经熟悉404 状态码，它表示“没找到”）。
如果请求成功，下载的页面就作为一个字符串，保存在Response 对象的text变量中。这个变量保存了包含整部戏剧的一个大字符串，调用len(res.text)表明，
它的长度超过178000 个字符。最后，调用print(res.text[:250])显示前250 个字符。
检查错误
正如你看到的，Response 对象有一个status_code 属性，可以检查它是否等于requests.codes.ok，了解下载是否成功。检查成功有一种简单的方法，就是在
Response对象上调用raise_for_status()方法。如果下载文件出错，这将抛出异常。如果下载成功，就什么也不做。在交互式环境中输入以下代码：
>>> res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
>>> res.raise_for_status()
Traceback (most recent call last):
File "<pyshell#138>", line 1, in <module>
res.raise_for_status()
File "C:\Python34\lib\site-packages\requests\models.py", line 773, in raise_for_status
raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found
raise_for_status()方法是一种很好的方式，确保程序在下载失败时停止。这是一件好事：你希望程序在发生未预期的错误时，马上停止。如果下载失败对程序来说
不够严重，可以用try 和except 语句将raise_for_status()代码行包裹起来，处理这一错误，不让程序崩溃。
import requests
res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
try:
res.raise_for_status()
except Exception as exc:
print('There was a problem: %s' % (exc))
这次raise_for_status()方法调用导致程序输出以下内容：
There was a problem: 404 Client Error: Not Found
总是在调用requests.get()之后再调用raise_for_status()。你希望确保下载确实成功，然后再让程序继续。
将下载的文件保存到硬盘
现在，可以用标准的open()函数和write()方法，将Web 页面保存到硬盘中的一个文件。但是，这里稍稍有一点不同。首先，必须用“写二进制”模式打开该文件，
即向函数传入字符串'wb'，作为open()的第二参数。即使该页面是纯文本的（例如前面下载的罗密欧与朱丽叶的文本），你也需要写入二进制数据，而不是文本数据，
目的是为了保存该文本中的“Unicode 编码”。
为了将Web 页面写入到一个文件，可以使用for 循环和Response 对象的iter_content()方法。
>>> import requests
>>> res = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt')
>>> res.raise_for_status()
>>> playFile = open('RomeoAndJuliet.txt', 'wb')
>>> for chunk in res.iter_content(100000):
playFile.write(chunk)
100000
78981
>>> playFile.close()
iter_content()方法在循环的每次迭代中，返回一段内容。每一段都是bytes 数据类型，你需要指定一段包含多少字节。10 万字节通常是不错的选择，所以将
100000作为参数传递给iter_content()。文件RomeoAndJuliet.txt 将存在于当前工作目录。请注意，虽然在网站上文件名是pg1112.txt，但在你的硬盘上，
该文件的名字不同。requests 模块只处理下载网页内容。一旦网页下载后，它就只是程序中的数据。即使在下载该网页后断开了因特网连接，该页面的所有数据
仍然会在你的计算机中。write()方法返回一个数字，表示写入文件的字节数。在前面的例子中，第一段包含100000 个字节，文件剩下的部分只需要78981 个字节。
回顾一下，下载并保存到文件的完整过程如下：
1．调用requests.get()下载该文件。
2．用'wb'调用open()，以写二进制的方式打开一个新文件。
3．利用Respose 对象的iter_content()方法做循环。
4．在每次迭代中调用write()，将内容写入该文件。
5．调用close()关闭该文件。
这就是关于requests 模块的全部内容！相对于写入文本文件的open()/write()/close()工作步骤，for 循环和iter_content()的部分可能看起来比较复杂，
但这是为了确保requests 模块即使在下载巨大的文件时也不会消耗太多内存。你可以访问http://requests.readthedocs.org/，了解requests 模块的其他功能。

项目13：“I’m Feeling Lucky”Google 查找
每次我在 Google 上搜索一个主题时，都不会一次只看一个搜索结果。通过鼠标中键点击搜索结果链接，或在点击时按住CTRL 键，我会在一些新的选项卡中打
开前几个链接，稍后再来查看。我经常搜索Google，所以这个工作流程（开浏览器，查找一个主题，依次用中键点击几个链接）变得很乏味。如果我只要在命令
行中输入查找主题，就能让计算机自动打开浏览器，并在新的选项卡中显示前面几项查询结果，那就太好了。让我们写一个脚本来完成这件事。
下面是程序要做的事：
• 从命令行参数中获取查询关键字。
• 取得查询结果页面。
• 为每个结果打开一个浏览器选项卡。
这意味着代码需要完成以下工作：
• 从sys.argv 中读取命令行参数。
• 用requests 模块取得查询结果页面。
• 找到每个查询结果的链接。
• 调用webbrowser.open()函数打开Web 浏览器。
打开一个新的文件编辑器窗口，并保存为lucky.py。
第１步：获取命令行参数，并请求查找页面
开始编码之前，你首先要知道查找结果页面的URL。在进行Google 查找后，你看浏览器地址栏，就会发现结果页面的URL 类似于https://www.google.com/
search?q=SEARCH_TERM_HERE。requests 模块可以下载这个页面，然后可以用Beautiful Soup，找到HTML 中的查询结果的链接。最后，用webbrowser 模块，
在浏览器选项卡中打开这些链接。让你的代码看起来像这样：
#! python3
# lucky.py - Opens several Google search results.
import requests, sys, webbrowser, bs4
print('Googling...') # display text while downloading the Google page
res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()
# TODO: Retrieve top search result links.
# TODO: Open a browser tab for each result.
用户运行该程序时，将通过命令行参数指定查找的主题。这些参数将作为字符串，保存在sys.argv 列表中。
第２步：找到所有的结果
现在你需要使用Beautiful Soup，从下载的HTML 中，提取排名靠前的查找结果链接。但如何知道完成这项工作需要怎样的选择器？例如，你不能只查找所有的<a>标
签，因为在这个HTML 中，有许多链接你是不关心的。因此，必须用浏览器的开发者工具来检查这个查找结果页面，尝试寻找一个选择器，它将挑选出你想要的链接。
在针对Beautiful Soup 进行Google 查询后，你可以打开浏览器的开发者工具，查看该页面上的一些链接元素。它们看起来复杂得难以置信，大概像这样：<a
href="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCgQFjAA&amp;url=http%3A%2F%2
Fwww.crummy.com%2Fsoftware%2FBeautifulSoup%2F&amp;ei=LHBVU_XDD9KVyAShmYDwCw&amp;usg=AFQjCNHAxwplurFOBqg5cehWQEVKi-TuLQ&amp;sig2=sd
Zu6WVlBlVSDrwhtworMA" onmousedown="return rwt(this,'','','','1','AFQjCNH AxwplurFOBqg5cehWQEVKi-TuLQ','sdZu6WVlBlVSDrwhtworMA','0CC
gQFjAA','','',event)" data-href="http://www.crummy.com/software/BeautifulSoup/"><em> BeautifulSoup</em>: We called him Tortoise
because he taught us.</a>
该元素看起来复杂得难以置信，但这没有关系。只需要找到查询结果链接都具有的模式。但这个<a>元素没有什么特殊，难以和该页面上非查询结果的<a>元素区分开来。
确保你的代码看起来像这样：
#! python3
# lucky.py - Opens several google search results.
import requests, sys, webbrowser, bs4
--snip--
# Retrieve top search result links.
soup = bs4.BeautifulSoup(res.text)
# Open a browser tab for each result.
linkElems = soup.select('.r a')
但是，如果从<a>元素向上看一点，就会发现这样一个元素：<h3 class="r">。查看余下的HTML 源代码，看起来r 类仅用于查询结果链接。你不需要知道CSS 类
r 是什么，或者它会做什么。只需要利用它作为一个标记，查找需要的<a>元素。可以通过下载页面的HTML 文本，创建一个BeautifulSoup 对象，然后用选择
符'.r a'，找到所有具有CSS 类r 的元素中的<a>元素。
第3 步：针对每个结果打开Web 浏览器
最后，我们将告诉程序，针对结果打开Web 浏览器选项卡。将下面的内容添加到程序的末尾：
#! python3
# lucky.py - Opens several google search results.
import requests, sys, webbrowser, bs4
--snip--
# Open a browser tab for each result.
linkElems = soup.select('.r a')
numOpen = min(5, len(linkElems))
for i in range(numOpen):
webbrowser.open('http://google.com' + linkElems[i].get('href'))
默认情况下，你会使用webbrowser 模块，在新的选项卡中打开前5 个查询结果。但是，用户查询的主题可能少于5 个查询结果。soup.select()调用返回一个列表
，包含匹配'.r a'选择器的所有元素，所以打开选项卡的数目要么是5，要么是这个列表的长度（取决于哪一个更小）。内建的Python 函数min()返回传入的整型或
浮点型参数中最小的一个（也有内建的max()函数，返回传入的参数中最大的一个）。你可以使用min()弄清楚该列表中是否少于5 个链接，并且将要打开的链接数保
存在变量numOpen 中。然后可以调用range(numOpen)，执行一个for 循环。在该循环的每次迭代中，你使用webbrowser.open()，在Web 浏览器中打开一个
新的选项卡。请注意，返回的<a>元素的href 属性中，不包含初始的http://google.com部分，所以必须连接它和href 属性的字符串。
现在可以马上打开前5 个Google 查找结果，比如说，要查找Python programmingtutorials，你只要在命令行中运行lucky python programming tutorials
第4 步：类似程序的想法
分选项卡浏览的好处在于，很容易在新选项卡中打开一些链接，稍后再来查看。一个自动打开几个链接的程序，很适合快捷地完成下列任务：
• 查找亚马逊这样的电商网站后，打开所有的产品页面；
• 打开针对一个产品的所有评论的链接；
• 查找Flickr 或Imgur 这样的照片网站后，打开查找结果中的所有照片的链接。

项目14：下载所有XKCD 漫画
博客和其他经常更新的网站通常有一个首页，其中有最新的帖子，以及一个“前一篇”按钮，将你带到以前的帖子。然后那个帖子也有一个“前一篇”按钮，以此
类推。这创建了一条线索，从最近的页面，直到该网站的第一个帖子。如果你希望拷贝该网站的内容，在离线的时候阅读，可以手工导航至每个页面并保存。但这是
很无聊的工作，所以让我们写一个程序来做这件事。XKCD 是一个流行的极客漫画网站，它符合这个结构（参见图 11-6）。首页http://xkcd.com/有一个“Prev”
按钮，让用户导航到前面的漫画。手工下载每张漫画要花较长的时间，但你可以写一个脚本，在几分钟内完成这件事。下面是程序要做的事：
• 加载主页；
• 保存该页的漫画图片；
• 转入前一张漫画的链接；
• 重复直到第一张漫画。
这意味着代码需要做下列事情：
• 利用requests 模块下载页面。
• 利用Beautiful Soup 找到页面中漫画图像的URL。
• 利用iter_content()下载漫画图像，并保存到硬盘。
• 找到前一张漫画的链接URL，然后重复。
打开一个新的文件编辑器窗口，将它保存为downloadXkcd.py。
第1 步：设计程序
打开一个浏览器的开发者工具，检查该页面上的元素，你会发现下面的内容：
• 漫画图像文件的URL，由一个<img>元素的href 属性给出。
• <img>元素在<div id="comic">元素之内。
• Prev 按钮有一个rel HTML 属性，值是prev。
• 第一张漫画的Prev 按钮链接到http://xkcd.com/# URL，表明没有前一个页面了。
让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
url = 'http://xkcd.com' # starting url
os.makedirs('xkcd', exist_ok=True) # store comics in ./xkcd
while not url.endswith('#'):
# TODO: Download the page.
# TODO: Find the URL of the comic image.
# TODO: Download the image.
# TODO: Save the image to ./xkcd.
# TODO: Get the Prev button's url.
print('Done.')
你会有一个url 变量，开始的值是'http://xkcd.com'，然后反复更新（在一个for循环中），变成当前页面的Prev 链接的URL。在循环的每一步，你将下载URL 上
的漫画。如果URL 以'#'结束，你就知道需要结束循环。将图像文件下载到当前目录的一个名为xkcd 的文件夹中。调用os.makedirs()函数。确保这个文件夹存在，
并且关键字参数exist_ok=True 在该文件夹已经存在时，防止该函数抛出异常。剩下的代码只是注释，列出了剩下程序的大纲。
第2 步：下载网页
我们来实现下载网页的代码。让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
url = 'http://xkcd.com' # starting url
os.makedirs('xkcd', exist_ok=True) # store comics in ./xkcd
while not url.endswith('#'):
# Download the page.
print('Downloading page %s...' % url)
res = requests.get(url)
res.raise_for_status()
soup = bs4.BeautifulSoup(res.text)
# TODO: Find the URL of the comic image.
# TODO: Download the image.
# TODO: Save the image to ./xkcd.
# TODO: Get the Prev button's url.
print('Done.')
首先，打印url，这样用户就知道程序将要下载哪个URL。然后利用requests 模块的request.get()函数下载它。像以往一样，马上调用Response 对象的
raise_for_status()方法，如果下载发生问题，就抛出异常，并终止程序。否则，利用下载页面的文本创建一个BeautifulSoup 对象。
第3 步：寻找和下载漫画图像
让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
--snip--
# Find the URL of the comic image.
comicElem = soup.select('#comic img')
if comicElem == []:
print('Could not find comic image.')
else:
comicUrl = 'http:' comicElem[0].get('src')
# Download the image.
print('Downloading image %s...' % (comicUrl))
res = requests.get(comicUrl)
res.raise_for_status()
# TODO: Save the image to ./xkcd.
# TODO: Get the Prev button's url.
print('Done.')
用开发者工具检查XKCD 主页后，你知道漫画图像的<img>元素是在一个<div>元素中，它带有的id 属性设置为comic。所以选择器'#comic img'将从BeautifulSoup
对象中选出正确的<img>元素。有一些XKCD 页面有特殊的内容，不是一个简单的图像文件。这没问题，跳过它们就好了。如果选择器没有找到任何元素，那么
soup.select('#comic img')将返回一个空的列表。出现这种情况时，程序将打印一条错误消息，不下载图像，继续执行。否则，选择器将返回一个列表，包含一
个<img>元素。可以从这个<img>元素中取得src 属性，将它传递给requests.get()，下载这个漫画的图像文件。
第4 步：保存图像，找到前一张漫画
让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
--snip--
# Save the image to ./xkcd.
imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')
for chunk in res.iter_content(100000):
imageFile.write(chunk)
imageFile.close()
# Get the Prev button's url.
prevLink = soup.select('a[rel="prev"]')[0]
url = 'http://xkcd.com' + prevLink.get('href')
print('Done.')
第11 章 从Web 抓取信息 209
这时，漫画的图像文件保存在变量res 中。你需要将图像数据写入硬盘的文件。你需要为本地图像文件准备一个文件名，传递给 open()。comicUrl 的值类似
'http://imgs.xkcd.com/comics/heartbleed_explanation.png'。你可能注意到，它看起来很像文件路径。实际上，调用os.path.basename()时传入comicUrl，
它只返回URL 的最后部分：'heartbleed_explanation.png'。你可以用它作为文件名，将图像保存到硬盘。用os.path.join()连接这个名称和xkcd 文件夹的名称，
这样程序就会在Windows下使用倒斜杠（\），在OS X 和Linux 下使用斜杠（/）。既然你最后得到了文件名，就可以调用open()，用'wb'模式打开一个新文件。
回忆一下本章早些时候，保存利用 Requests 下载的文件时，你需要循环处理iter_content()方法的返回值。for 循环中的代码将一段图像数据写入文件（每次最多
10 万字节），然后关闭该文件。图像现在保存到硬盘中。然后，选择器'a[rel="prev"]'识别出rel 属性设置为prev 的<a>元素，利用这个<a>元素的href 属性，
取得前一张漫画的URL，将它保存在url 中。然后while 循环针对这张漫画，再次开始整个下载过程。这个程序的输出看起来像这样：
Downloading page http://xkcd.com...
Downloading image http://imgs.xkcd.com/comics/phone_alarm.png...
Downloading page http://xkcd.com/1358/...
Downloading image http://imgs.xkcd.com/comics/nro.png...
Downloading page http://xkcd.com/1357/...
Downloading image http://imgs.xkcd.com/comics/free_speech.png...
Downloading page http://xkcd.com/1356/...
Downloading image http://imgs.xkcd.com/comics/orbital_mechanics.png...
Downloading page http://xkcd.com/1355/...
Downloading image http://imgs.xkcd.com/comics/airplane_message.png...
Downloading page http://xkcd.com/1354/...
Downloading image http://imgs.xkcd.com/comics/heartbleed_explanation.png...
--snip--
这个项目是一个很好的例子，说明程序可以自动顺着链接，从网络上抓取大量的数据。你可以从Beautiful Soup 的文档了解它的更多功能：
http://www. crummy.com/software/BeautifulSoup/bs4/doc/.
第5 步：类似程序的想法
下载页面并追踪链接，是许多网络爬虫程序的基础。类似的程序也可以做下面的事情：
• 顺着网站的所有链接，备份整个网站。
• 拷贝一个论坛的所有信息。
• 复制一个在线商店中所有产品的目录。
requests 和 BeautifulSoup 模块很了不起，只要你能弄清楚需要传递给requests.get()的URL。但是，有时候这并不容易找到。或者，你希望编程浏览的网站
可能要求你先登录。selenium 模块将让你的程序具有执行这种复杂任务的能力。
